{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando landmarks - ResNet18 - GridSearch(epoch vs batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VZMYlBcQiq0"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run on aws ec2 machine (conda_tersorflow_p36 kernel)\n",
    "# !pip install tensorflow==1.13.1\n",
    "# !pip install image-classifiers\n",
    "# !pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "UyeuHtlZRPnw",
    "outputId": "ef7017b1-cdf7-42ec-b1a2-605707f29e1f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import callbacks, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4127605359904207207\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12705287089786501085\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6732846262141161139\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330115994\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10648547885565590165\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRGnhFs2iLaB"
   },
   "source": [
    "## Classificando Landmarks (Análise dos dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8BwpFbhyanV"
   },
   "source": [
    "### Lendo o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "cZsAwFpERdZK",
    "outputId": "81abc47f-ee3e-49f2-9e7c-27a92967420e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12508 images belonging to 20 classes.\n",
      "Found 3128 images belonging to 20 classes.\n",
      "\n",
      "\n",
      "Showing y sample: [ 8.  6.  8.  3.  8. 16. 18.  8.  2. 19.  3.  7.  1. 16. 11. 15. 17.  5.\n",
      "  2. 14.  5. 15.  1. 19. 15.  2.  7.  4.  9. 18.  8.  2.]\n",
      "\n",
      "\n",
      "samples in train: 12508\n",
      "samples in test: 3128\n",
      "features: (224, 224, 3)\n",
      "classes: 20\n",
      "\n",
      "shape: (32, 224, 224, 3) (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "seed = random.seed(42)\n",
    "\n",
    "sample_datagen = ImageDataGenerator(rescale=1./255)\n",
    "base_path = '/home/ubuntu/landmarks/landmarks'\n",
    "target_size = (224, 224)\n",
    "input_shape = (224, 224, 3)\n",
    "classes = [\"47378\", \"120885\", \"85758\", \"180901\", \"48522\", \"101399\", \n",
    "           \"190822\", \"97734\", \"146250\", \"186080\", \"21253\", \"142644\", \n",
    "           \"31531\", \"165596\", \"56827\", \"38482\", \"20102\", \"178519\", \n",
    "           \"152827\", \"173511\"]\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "sample_generator = sample_datagen.flow_from_directory(base_path + '/subset_train',\n",
    "                                                      target_size=target_size,\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode=\"sparse\",\n",
    "                                                      seed = seed)\n",
    "\n",
    "sample_test_generator = sample_datagen.flow_from_directory(base_path + \"/subset_test\",\n",
    "                                                           target_size = target_size,\n",
    "                                                           batch_size = 32,\n",
    "                                                           class_mode = \"categorical\",\n",
    "                                                           seed = seed)\n",
    "\n",
    "total_classes = np.max(sample_generator.labels) + 1\n",
    "\n",
    "x_sample, y_sample = sample_generator.next()\n",
    "x_sample_test, y_sample_test = sample_test_generator.next()\n",
    "print('\\n')\n",
    "print('Showing y sample:', y_sample)\n",
    "print('\\n')\n",
    "print('samples in train: %i' % sample_generator.labels.shape,\n",
    "      'samples in test: %i' % sample_test_generator.labels.shape,\n",
    "      'features: %s' % str(x_sample.shape[1:]),\n",
    "      'classes: %i' % total_classes,\n",
    "      sep='\\n', end='\\n\\n')\n",
    "\n",
    "print('shape:', x_sample.shape, x_sample_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xvoRGvyzONU"
   },
   "source": [
    "## Treinamento \n",
    "### Parâmetros para treinamento e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BO1JgkEHu93m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "rms = optimizers.RMSprop(lr = 0.0002,\n",
    "                         decay = 1e-6)\n",
    "\n",
    "device = '/gpu:0'\n",
    "\n",
    "epochs = 64\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kQEZz7Pfk6n"
   },
   "source": [
    "## Funções de auxílio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TerminateOnNaN, ReduceLROnPlateau\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import types\n",
    "\n",
    "## Adapted from taken from keras.wrappers.scikit_learn.KerasClassifier.fit \n",
    "class KerasBatchClassifier(KerasClassifier):\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "\n",
    "        # taken from keras.wrappers.scikit_learn.KerasClassifier.fit ###################################################\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType) and not isinstance(self.build_fn, types.MethodType):\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "\n",
    "        ################################################################################################################\n",
    "        epochs = self.sk_params['epochs'] if 'epochs' in self.sk_params else 100\n",
    "        batch = self.sk_params['batch_size'] if 'batch_size' in self.sk_params else 32\n",
    "        print('[Debug] - epochs=', epochs)\n",
    "        print('[Debug] - batch=', batch)\n",
    "        \n",
    "        patience = epochs // 3\n",
    "        \n",
    "        base_path = kwargs['base_path']\n",
    "        target_size = kwargs['target_size']\n",
    "        \n",
    "        datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "\n",
    "        self.validation_flow = datagen.flow_from_directory(\n",
    "            base_path + \"/subset_train\",\n",
    "            target_size = target_size,\n",
    "            batch_size = batch,\n",
    "            class_mode = \"categorical\",\n",
    "            subset ='validation')\n",
    "        \n",
    "        self.validation_steps = self.validation_flow.samples // batch\n",
    "        \n",
    "        train_flow = datagen.flow_from_directory(\n",
    "            base_path + \"/subset_train\",\n",
    "            target_size = target_size,\n",
    "            batch_size = batch,\n",
    "            class_mode = \"categorical\",\n",
    "            subset = 'training')\n",
    "        \n",
    "        train_steps = train_flow.samples // batch\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\"results/best_weights.{epoch:02d}-{loss:.5f}.hdf5\", \n",
    "                                           verbose=1, \n",
    "                                           save_best_only=True,\n",
    "                                           mode=\"auto\")\n",
    "        terminate_onnan = TerminateOnNaN()\n",
    "        reduce_plateau = ReduceLROnPlateau(patience=patience)\n",
    "        \n",
    "        callbacks = [model_checkpoint, terminate_onnan, reduce_plateau]\n",
    "\n",
    "        self.__history = self.model.fit_generator(\n",
    "            train_flow,  \n",
    "            steps_per_epoch=train_steps,\n",
    "            validation_data=self.validation_flow, \n",
    "            validation_steps=self.validation_steps, \n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "\n",
    "        return self.__history\n",
    "\n",
    "    def score(self, X, y=None, **kwargs):\n",
    "        outputs = self.model.evaluate_generator(self.validation_flow, self.validation_steps)\n",
    "        if type(outputs) is not list:\n",
    "            outputs = [outputs]\n",
    "        for name, output in zip(self.model.metrics_names, outputs):\n",
    "            if name == 'acc':\n",
    "                return output\n",
    "        raise Exception('The model is not configured to compute accuracy. '\n",
    "                        'You should pass `metrics=[\"accuracy\"]` to '\n",
    "                        'the `model.compile()` method.')\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.__history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejdscnncgQs4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "def grid_search(create_model, param_grid, train_epochs = epochs):\n",
    "    model = KerasBatchClassifier(build_fn=create_model)\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, \n",
    "                        param_grid=param_grid, \n",
    "                        cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0))\n",
    "    with tf.device(device):\n",
    "        return grid.fit((1, 1, 1), base_path = base_path, target_size = target_size, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqegaNRbJ5x_"
   },
   "source": [
    "### Definindo a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FD28OW8F1nXw"
   },
   "outputs": [],
   "source": [
    "from classification_models.resnet import ResNet18, preprocess_input\n",
    "\n",
    "def build_resNet18(optimizer = rms):\n",
    "    model = ResNet18(input_shape = input_shape,\n",
    "                   weights = \"imagenet\",\n",
    "                   include_top=False)\n",
    "\n",
    "    for layer in model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "    output = model.output\n",
    "\n",
    "    output = Flatten(name = 'flat_mdc')(output)\n",
    "\n",
    "    output = Dense(total_classes,\n",
    "                   activation ='softmax',\n",
    "                   name = 'saida_mdc')(output)\n",
    "\n",
    "    model = Model(inputs = model.input, outputs = output)\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy', \n",
    "                  optimizer = optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LjvsKG417IT"
   },
   "outputs": [],
   "source": [
    "def build_resNet18_tuning():\n",
    "    model = build_resNet18()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy',\n",
    "                  optimizer = rms,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqdPAGMJ4Sn8"
   },
   "source": [
    "### GridSearch 1 - batch size vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1CuiCbX4hHq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] - epochs= 4\n",
      "[Debug] - batch= 30\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "333/333 [==============================] - 123s 371ms/step - loss: 2.2516 - acc: 0.5580 - val_loss: 3.7530 - val_acc: 0.5193\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.75297, saving model to results/best_weights.01-2.25160.hdf5\n",
      "Epoch 2/4\n",
      "333/333 [==============================] - 118s 353ms/step - loss: 0.8549 - acc: 0.7844 - val_loss: 3.0861 - val_acc: 0.6121\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.75297 to 3.08606, saving model to results/best_weights.02-0.85483.hdf5\n",
      "Epoch 3/4\n",
      "333/333 [==============================] - 117s 352ms/step - loss: 0.5374 - acc: 0.8654 - val_loss: 3.2236 - val_acc: 0.6527\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.08606\n",
      "Epoch 4/4\n",
      "333/333 [==============================] - 117s 351ms/step - loss: 0.1680 - acc: 0.9601 - val_loss: 0.8651 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.08606 to 0.86511, saving model to results/best_weights.04-0.16800.hdf5\n",
      "[Debug] - epochs= 8\n",
      "[Debug] - batch= 30\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/8\n",
      "333/333 [==============================] - 123s 369ms/step - loss: 2.1029 - acc: 0.4290 - val_loss: 1.3443 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34431, saving model to results/best_weights.01-2.10293.hdf5\n",
      "Epoch 2/8\n",
      "333/333 [==============================] - 117s 351ms/step - loss: 0.7658 - acc: 0.7740 - val_loss: 1.0860 - val_acc: 0.6828\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34431 to 1.08602, saving model to results/best_weights.02-0.76558.hdf5\n",
      "Epoch 3/8\n",
      "333/333 [==============================] - 117s 353ms/step - loss: 0.3430 - acc: 0.9074 - val_loss: 1.0009 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.08602 to 1.00093, saving model to results/best_weights.03-0.34287.hdf5\n",
      "Epoch 4/8\n",
      "333/333 [==============================] - 120s 360ms/step - loss: 0.1414 - acc: 0.9704 - val_loss: 0.9344 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00093 to 0.93443, saving model to results/best_weights.04-0.14134.hdf5\n",
      "Epoch 5/8\n",
      "333/333 [==============================] - 118s 355ms/step - loss: 0.0558 - acc: 0.9933 - val_loss: 0.9431 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.93443\n",
      "Epoch 6/8\n",
      "333/333 [==============================] - 118s 353ms/step - loss: 0.0242 - acc: 0.9989 - val_loss: 0.9243 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.93443 to 0.92428, saving model to results/best_weights.06-0.02419.hdf5\n",
      "Epoch 7/8\n",
      "333/333 [==============================] - 119s 357ms/step - loss: 0.0133 - acc: 0.9990 - val_loss: 0.9621 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.92428\n",
      "Epoch 8/8\n",
      "333/333 [==============================] - 117s 351ms/step - loss: 0.0075 - acc: 0.9997 - val_loss: 0.9553 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.92428\n",
      "[Debug] - epochs= 12\n",
      "[Debug] - batch= 30\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/12\n",
      "333/333 [==============================] - 122s 368ms/step - loss: 3.8180 - acc: 0.0921 - val_loss: 3.2151 - val_acc: 0.1691\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.21508, saving model to results/best_weights.01-3.81805.hdf5\n",
      "Epoch 2/12\n",
      "333/333 [==============================] - 118s 353ms/step - loss: 2.8047 - acc: 0.2372 - val_loss: 2.5452 - val_acc: 0.2994\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.21508 to 2.54522, saving model to results/best_weights.02-2.80445.hdf5\n",
      "Epoch 3/12\n",
      "333/333 [==============================] - 119s 358ms/step - loss: 2.2246 - acc: 0.3637 - val_loss: 2.1740 - val_acc: 0.3761\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.54522 to 2.17404, saving model to results/best_weights.03-2.22446.hdf5\n",
      "Epoch 4/12\n",
      "333/333 [==============================] - 117s 353ms/step - loss: 1.8620 - acc: 0.4562 - val_loss: 1.9311 - val_acc: 0.4387\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.17404 to 1.93107, saving model to results/best_weights.04-1.86191.hdf5\n",
      "Epoch 5/12\n",
      "333/333 [==============================] - 118s 354ms/step - loss: 1.5920 - acc: 0.5252 - val_loss: 1.7545 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.93107 to 1.75447, saving model to results/best_weights.05-1.59177.hdf5\n",
      "Epoch 6/12\n",
      "333/333 [==============================] - 115s 346ms/step - loss: 1.3931 - acc: 0.5814 - val_loss: 1.6298 - val_acc: 0.5223\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.75447 to 1.62978, saving model to results/best_weights.06-1.39297.hdf5\n",
      "Epoch 7/12\n",
      "333/333 [==============================] - 117s 352ms/step - loss: 1.2341 - acc: 0.6342 - val_loss: 1.5229 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.62978 to 1.52286, saving model to results/best_weights.07-1.23422.hdf5\n",
      "Epoch 8/12\n",
      "333/333 [==============================] - 118s 354ms/step - loss: 1.0952 - acc: 0.6714 - val_loss: 1.4696 - val_acc: 0.5642\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.52286 to 1.46963, saving model to results/best_weights.08-1.09524.hdf5\n",
      "Epoch 9/12\n",
      "333/333 [==============================] - 118s 355ms/step - loss: 0.9881 - acc: 0.7045 - val_loss: 1.3947 - val_acc: 0.5894\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.46963 to 1.39473, saving model to results/best_weights.09-0.98806.hdf5\n",
      "Epoch 10/12\n",
      "333/333 [==============================] - 119s 357ms/step - loss: 0.8756 - acc: 0.7373 - val_loss: 1.3303 - val_acc: 0.5983\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.39473 to 1.33028, saving model to results/best_weights.10-0.87551.hdf5\n",
      "Epoch 11/12\n",
      "333/333 [==============================] - 117s 350ms/step - loss: 0.7997 - acc: 0.7602 - val_loss: 1.2825 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.33028 to 1.28246, saving model to results/best_weights.11-0.79967.hdf5\n",
      "Epoch 12/12\n",
      "333/333 [==============================] - 115s 345ms/step - loss: 0.7136 - acc: 0.7913 - val_loss: 1.2640 - val_acc: 0.6292\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.28246 to 1.26397, saving model to results/best_weights.12-0.71351.hdf5\n",
      "[Debug] - epochs= 4\n",
      "[Debug] - batch= 60\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/4\n",
      "166/166 [==============================] - 127s 766ms/step - loss: 3.9395 - acc: 0.0837 - val_loss: 3.4838 - val_acc: 0.1215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.48384, saving model to results/best_weights.01-3.93954.hdf5\n",
      "Epoch 2/4\n",
      "166/166 [==============================] - 113s 682ms/step - loss: 3.0356 - acc: 0.1929 - val_loss: 2.8608 - val_acc: 0.2315\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.48384 to 2.86081, saving model to results/best_weights.02-3.03558.hdf5\n",
      "Epoch 3/4\n",
      "166/166 [==============================] - 115s 692ms/step - loss: 2.4859 - acc: 0.3087 - val_loss: 2.4368 - val_acc: 0.3236\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.86081 to 2.43681, saving model to results/best_weights.03-2.48563.hdf5\n",
      "Epoch 4/4\n",
      "166/166 [==============================] - 116s 698ms/step - loss: 2.0914 - acc: 0.4053 - val_loss: 2.2012 - val_acc: 0.3742\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.43681 to 2.20117, saving model to results/best_weights.04-2.09139.hdf5\n",
      "[Debug] - epochs= 8\n",
      "[Debug] - batch= 60\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/8\n",
      "166/166 [==============================] - 129s 777ms/step - loss: 3.8981 - acc: 0.0907 - val_loss: 3.4112 - val_acc: 0.1276\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.41123, saving model to results/best_weights.01-3.89814.hdf5\n",
      "Epoch 2/8\n",
      "166/166 [==============================] - 115s 693ms/step - loss: 3.0270 - acc: 0.1911 - val_loss: 2.8228 - val_acc: 0.2299\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.41123 to 2.82284, saving model to results/best_weights.02-3.02677.hdf5\n",
      "Epoch 3/8\n",
      "166/166 [==============================] - 118s 714ms/step - loss: 2.4810 - acc: 0.3022 - val_loss: 2.4278 - val_acc: 0.3232\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.82284 to 2.42778, saving model to results/best_weights.03-2.48093.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "166/166 [==============================] - 120s 723ms/step - loss: 2.0927 - acc: 0.3980 - val_loss: 2.1683 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.42778 to 2.16829, saving model to results/best_weights.04-2.09274.hdf5\n",
      "Epoch 5/8\n",
      "166/166 [==============================] - 122s 735ms/step - loss: 1.8152 - acc: 0.4710 - val_loss: 1.9945 - val_acc: 0.4359\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.16829 to 1.99449, saving model to results/best_weights.05-1.81525.hdf5\n",
      "Epoch 6/8\n",
      "166/166 [==============================] - 118s 709ms/step - loss: 1.6171 - acc: 0.5301 - val_loss: 1.8316 - val_acc: 0.4762\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.99449 to 1.83161, saving model to results/best_weights.06-1.61709.hdf5\n",
      "Epoch 7/8\n",
      "166/166 [==============================] - 119s 717ms/step - loss: 1.4380 - acc: 0.5710 - val_loss: 1.7568 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.83161 to 1.75677, saving model to results/best_weights.07-1.43788.hdf5\n",
      "Epoch 8/8\n",
      "166/166 [==============================] - 119s 718ms/step - loss: 1.3015 - acc: 0.6120 - val_loss: 1.6332 - val_acc: 0.5354\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.75677 to 1.63324, saving model to results/best_weights.08-1.30145.hdf5\n",
      "[Debug] - epochs= 12\n",
      "[Debug] - batch= 60\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/12\n",
      "166/166 [==============================] - 123s 744ms/step - loss: 3.8819 - acc: 0.0846 - val_loss: 3.4159 - val_acc: 0.1333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.41593, saving model to results/best_weights.01-3.88195.hdf5\n",
      "Epoch 2/12\n",
      "166/166 [==============================] - 112s 674ms/step - loss: 3.0300 - acc: 0.1873 - val_loss: 2.8234 - val_acc: 0.2368\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.41593 to 2.82335, saving model to results/best_weights.02-3.02996.hdf5\n",
      "Epoch 3/12\n",
      "166/166 [==============================] - 113s 683ms/step - loss: 2.4744 - acc: 0.3081 - val_loss: 2.4664 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.82335 to 2.46641, saving model to results/best_weights.03-2.47449.hdf5\n",
      "Epoch 4/12\n",
      "166/166 [==============================] - 115s 690ms/step - loss: 2.0934 - acc: 0.3986 - val_loss: 2.1599 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.46641 to 2.15985, saving model to results/best_weights.04-2.09332.hdf5\n",
      "Epoch 5/12\n",
      "166/166 [==============================] - 112s 675ms/step - loss: 1.7945 - acc: 0.4767 - val_loss: 2.0167 - val_acc: 0.4297\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.15985 to 2.01673, saving model to results/best_weights.05-1.79440.hdf5\n",
      "Epoch 6/12\n",
      "166/166 [==============================] - 115s 693ms/step - loss: 1.6010 - acc: 0.5313 - val_loss: 1.8409 - val_acc: 0.4671\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.01673 to 1.84090, saving model to results/best_weights.06-1.60080.hdf5\n",
      "Epoch 7/12\n",
      "166/166 [==============================] - 117s 705ms/step - loss: 1.4275 - acc: 0.5784 - val_loss: 1.7518 - val_acc: 0.4934\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.84090 to 1.75178, saving model to results/best_weights.07-1.42759.hdf5\n",
      "Epoch 8/12\n",
      "166/166 [==============================] - 113s 682ms/step - loss: 1.2899 - acc: 0.6189 - val_loss: 1.6489 - val_acc: 0.5201\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.75178 to 1.64892, saving model to results/best_weights.08-1.28997.hdf5\n",
      "Epoch 9/12\n",
      "166/166 [==============================] - 116s 701ms/step - loss: 1.1477 - acc: 0.6638 - val_loss: 1.5649 - val_acc: 0.5403\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.64892 to 1.56487, saving model to results/best_weights.09-1.14777.hdf5\n",
      "Epoch 10/12\n",
      "166/166 [==============================] - 117s 705ms/step - loss: 1.0561 - acc: 0.6898 - val_loss: 1.5249 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.56487 to 1.52492, saving model to results/best_weights.10-1.05594.hdf5\n",
      "Epoch 11/12\n",
      "166/166 [==============================] - 114s 687ms/step - loss: 0.9636 - acc: 0.7198 - val_loss: 1.4601 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.52492 to 1.46010, saving model to results/best_weights.11-0.96359.hdf5\n",
      "Epoch 12/12\n",
      "166/166 [==============================] - 117s 705ms/step - loss: 0.8738 - acc: 0.7484 - val_loss: 1.4001 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.46010 to 1.40010, saving model to results/best_weights.12-0.87378.hdf5\n",
      "[Debug] - epochs= 4\n",
      "[Debug] - batch= 120\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/4\n",
      "83/83 [==============================] - 128s 2s/step - loss: 3.9667 - acc: 0.0723 - val_loss: 3.5378 - val_acc: 0.1004\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.53776, saving model to results/best_weights.01-3.96665.hdf5\n",
      "Epoch 2/4\n",
      "83/83 [==============================] - 107s 1s/step - loss: 3.3009 - acc: 0.1354 - val_loss: 3.0749 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.53776 to 3.07489, saving model to results/best_weights.02-3.29635.hdf5\n",
      "Epoch 3/4\n",
      "83/83 [==============================] - 112s 1s/step - loss: 2.8153 - acc: 0.2216 - val_loss: 2.6934 - val_acc: 0.2660\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.07489 to 2.69341, saving model to results/best_weights.03-2.81226.hdf5\n",
      "Epoch 4/4\n",
      "83/83 [==============================] - 115s 1s/step - loss: 2.4598 - acc: 0.3055 - val_loss: 2.4379 - val_acc: 0.3267\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.69341 to 2.43793, saving model to results/best_weights.04-2.45819.hdf5\n",
      "[Debug] - epochs= 8\n",
      "[Debug] - batch= 120\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/8\n",
      "83/83 [==============================] - 129s 2s/step - loss: 3.9544 - acc: 0.0791 - val_loss: 3.5796 - val_acc: 0.1038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.57963, saving model to results/best_weights.01-3.95439.hdf5\n",
      "Epoch 2/8\n",
      "83/83 [==============================] - 107s 1s/step - loss: 3.3214 - acc: 0.1400 - val_loss: 3.1103 - val_acc: 0.1686\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.57963 to 3.11025, saving model to results/best_weights.02-3.31829.hdf5\n",
      "Epoch 3/8\n",
      "83/83 [==============================] - 113s 1s/step - loss: 2.8369 - acc: 0.2217 - val_loss: 2.7345 - val_acc: 0.2475\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.11025 to 2.73455, saving model to results/best_weights.03-2.83621.hdf5\n",
      "Epoch 4/8\n",
      "83/83 [==============================] - 113s 1s/step - loss: 2.4773 - acc: 0.2991 - val_loss: 2.4844 - val_acc: 0.3035\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.73455 to 2.48444, saving model to results/best_weights.04-2.47472.hdf5\n",
      "Epoch 5/8\n",
      "83/83 [==============================] - 115s 1s/step - loss: 2.1786 - acc: 0.3775 - val_loss: 2.2892 - val_acc: 0.3360\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.48444 to 2.28918, saving model to results/best_weights.05-2.17937.hdf5\n",
      "Epoch 6/8\n",
      "83/83 [==============================] - 114s 1s/step - loss: 1.9499 - acc: 0.4367 - val_loss: 2.1096 - val_acc: 0.3912\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.28918 to 2.10961, saving model to results/best_weights.06-1.94864.hdf5\n",
      "Epoch 7/8\n",
      "83/83 [==============================] - 117s 1s/step - loss: 1.7553 - acc: 0.4839 - val_loss: 2.0065 - val_acc: 0.4127\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.10961 to 2.00648, saving model to results/best_weights.07-1.75242.hdf5\n",
      "Epoch 8/8\n",
      "83/83 [==============================] - 112s 1s/step - loss: 1.5967 - acc: 0.5313 - val_loss: 1.8308 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.00648 to 1.83084, saving model to results/best_weights.08-1.59677.hdf5\n",
      "[Debug] - epochs= 12\n",
      "[Debug] - batch= 120\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/12\n",
      "83/83 [==============================] - 130s 2s/step - loss: 4.1765 - acc: 0.0683 - val_loss: 3.7707 - val_acc: 0.0946\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.77073, saving model to results/best_weights.01-4.17645.hdf5\n",
      "Epoch 2/12\n",
      "83/83 [==============================] - 107s 1s/step - loss: 3.4590 - acc: 0.1250 - val_loss: 3.3038 - val_acc: 0.1463\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.77073 to 3.30379, saving model to results/best_weights.02-3.45442.hdf5\n",
      "Epoch 3/12\n",
      "83/83 [==============================] - 114s 1s/step - loss: 2.9531 - acc: 0.2022 - val_loss: 2.9023 - val_acc: 0.2163\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.30379 to 2.90233, saving model to results/best_weights.03-2.95418.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12\n",
      "83/83 [==============================] - 113s 1s/step - loss: 2.5570 - acc: 0.2795 - val_loss: 2.6150 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.90233 to 2.61500, saving model to results/best_weights.04-2.55686.hdf5\n",
      "Epoch 5/12\n",
      "83/83 [==============================] - 110s 1s/step - loss: 2.2577 - acc: 0.3527 - val_loss: 2.4036 - val_acc: 0.3284\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.61500 to 2.40362, saving model to results/best_weights.05-2.25341.hdf5\n",
      "Epoch 6/12\n",
      "83/83 [==============================] - 113s 1s/step - loss: 2.0167 - acc: 0.4140 - val_loss: 2.1719 - val_acc: 0.3895\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.40362 to 2.17195, saving model to results/best_weights.06-2.01466.hdf5\n",
      "Epoch 7/12\n",
      "83/83 [==============================] - 116s 1s/step - loss: 1.8057 - acc: 0.4710 - val_loss: 2.1318 - val_acc: 0.3988\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.17195 to 2.13185, saving model to results/best_weights.07-1.80561.hdf5\n",
      "Epoch 8/12\n",
      "83/83 [==============================] - 116s 1s/step - loss: 1.6516 - acc: 0.5113 - val_loss: 1.9014 - val_acc: 0.4659\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.13185 to 1.90137, saving model to results/best_weights.08-1.65127.hdf5\n",
      "Epoch 9/12\n",
      "83/83 [==============================] - 114s 1s/step - loss: 1.5053 - acc: 0.5538 - val_loss: 1.8823 - val_acc: 0.4659\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.90137 to 1.88232, saving model to results/best_weights.09-1.50435.hdf5\n",
      "Epoch 10/12\n",
      "83/83 [==============================] - 113s 1s/step - loss: 1.3833 - acc: 0.5904 - val_loss: 1.7732 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.88232 to 1.77318, saving model to results/best_weights.10-1.38013.hdf5\n",
      "Epoch 11/12\n",
      "83/83 [==============================] - 112s 1s/step - loss: 1.2846 - acc: 0.6222 - val_loss: 1.7375 - val_acc: 0.5004\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.77318 to 1.73751, saving model to results/best_weights.11-1.28157.hdf5\n",
      "Epoch 12/12\n",
      "83/83 [==============================] - 115s 1s/step - loss: 1.1784 - acc: 0.6508 - val_loss: 1.6666 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.73751 to 1.66661, saving model to results/best_weights.12-1.17895.hdf5\n",
      "[Debug] - epochs= 4\n",
      "[Debug] - batch= 240\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[512,512,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node stage4_unit1_conv2_9/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_19/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c95d18ede778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_resNet18_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2c519cc472c1>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(create_model, param_grid, train_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m                         cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0))\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c324d04fff56>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[512,512,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node stage4_unit1_conv2_9/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_19/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "batch_size = [30, 60, 120]\n",
    "epochs = [4, 8, 12, 24]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid_result = grid_search(build_resNet18_tuning, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaKoggmw4pc7"
   },
   "source": [
    "### Avaliando modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJDkdhhZ4ueT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-addede969c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "SdjnR8XFy8iI",
    "9aLhS6sV01tQ",
    "RTTCyhV8Cvr7"
   ],
   "name": "TCC_MDC_fruit_v4_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
