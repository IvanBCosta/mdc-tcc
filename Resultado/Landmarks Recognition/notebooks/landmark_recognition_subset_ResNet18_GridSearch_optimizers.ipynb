{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFHisozztLVy"
   },
   "source": [
    "# Avaliando landmarks - ResNet18 - GridSearch(optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VZMYlBcQiq0"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJysqbp3tLV8"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "_KW-8WVQtLWH",
    "outputId": "f4a23cd0-011c-428d-8c35-5c0f6b865440"
   },
   "outputs": [],
   "source": [
    "## Run on aws ec2 machine (conda_tersorflow_p36 kernel)\n",
    "# !pip install tensorflow==1.13.1\n",
    "# !pip install image-classifiers\n",
    "# !pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UyeuHtlZRPnw",
    "outputId": "eec0e5d3-6480-43c8-eb68-e690547372c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import callbacks, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XwKWYWkXjcxF",
    "outputId": "b908e098-85cb-459e-9e7f-7de8c8896c77"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "d8Pcuo45tLWU",
    "outputId": "66aa98c2-ef49-4986-e83c-c63c0ac42b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13133543556398069913\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15864951280737987088\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13221126437243513552\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330115994\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11004350561034749023\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRGnhFs2iLaB"
   },
   "source": [
    "## Classificando Landmarks (Análise dos dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8BwpFbhyanV"
   },
   "source": [
    "### Lendo o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "cZsAwFpERdZK",
    "outputId": "88a6b100-278e-477e-8ba8-6370d8198ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12508 images belonging to 20 classes.\n",
      "\n",
      "\n",
      "Showing y sample: [ 8.  6.  8.  3.  8. 16. 18.  8.  2. 19.  3.  7.  1. 16. 11. 15. 17.  5.\n",
      "  2. 14.  5. 15.  1. 19. 15.  2.  7.  4.  9. 18.  8.  2.]\n",
      "\n",
      "\n",
      "samples in train: 12508\n",
      "features: (224, 224, 3)\n",
      "classes: 20\n",
      "\n",
      "shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "seed = random.seed(42)\n",
    "\n",
    "sample_datagen = ImageDataGenerator(rescale=1./255)\n",
    "base_path = '/home/ubuntu/landmarks/landmarks'\n",
    "target_size = (224, 224)\n",
    "input_shape = (224, 224, 3)\n",
    "classes = [\"47378\", \"120885\", \"85758\", \"180901\", \"48522\", \"101399\", \n",
    "           \"190822\", \"97734\", \"146250\", \"186080\", \"21253\", \"142644\", \n",
    "           \"31531\", \"165596\", \"56827\", \"38482\", \"20102\", \"178519\", \n",
    "           \"152827\", \"173511\"]\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "sample_generator = sample_datagen.flow_from_directory(base_path + '/subset_train',\n",
    "                                                      target_size=target_size,\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode=\"sparse\",\n",
    "                                                      seed = seed)\n",
    "\n",
    "total_classes = np.max(sample_generator.labels) + 1\n",
    "\n",
    "x_sample, y_sample = sample_generator.next()\n",
    "print('\\n')\n",
    "print('Showing y sample:', y_sample)\n",
    "print('\\n')\n",
    "print('samples in train: %i' % sample_generator.labels.shape,\n",
    "      'features: %s' % str(x_sample.shape[1:]),\n",
    "      'classes: %i' % total_classes,\n",
    "      sep='\\n', end='\\n\\n')\n",
    "\n",
    "print('shape:', x_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xvoRGvyzONU"
   },
   "source": [
    "## Treinamento \n",
    "### Parâmetros para treinamento e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BO1JgkEHu93m"
   },
   "outputs": [],
   "source": [
    "device = '/gpu:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kQEZz7Pfk6n"
   },
   "source": [
    "## Funções de auxílio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3Sx9EwItLWz"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TerminateOnNaN, ReduceLROnPlateau\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import types\n",
    "\n",
    "## Adapted from taken from keras.wrappers.scikit_learn.KerasClassifier.fit \n",
    "class KerasBatchClassifier(KerasClassifier):\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "\n",
    "        # taken from keras.wrappers.scikit_learn.KerasClassifier.fit ###################################################\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType) and not isinstance(self.build_fn, types.MethodType):\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "\n",
    "        ################################################################################################################\n",
    "        epochs = self.sk_params['epochs'] if 'epochs' in self.sk_params else 100\n",
    "        batch = self.sk_params['batch_size'] if 'batch_size' in self.sk_params else 32\n",
    "        print('[Debug] - epochs=', epochs)\n",
    "        print('[Debug] - batch=', batch)\n",
    "        \n",
    "        patience = epochs // 3\n",
    "        \n",
    "        base_path = kwargs['base_path']\n",
    "        target_size = kwargs['target_size']\n",
    "        \n",
    "        datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "\n",
    "        self.validation_flow = datagen.flow_from_directory(\n",
    "            base_path + \"/subset_train\",\n",
    "            target_size = target_size,\n",
    "            batch_size = batch,\n",
    "            class_mode = \"categorical\",\n",
    "            subset ='validation')\n",
    "        \n",
    "        self.validation_steps = self.validation_flow.samples // batch\n",
    "        \n",
    "        train_flow = datagen.flow_from_directory(\n",
    "            base_path + \"/subset_train\",\n",
    "            target_size = target_size,\n",
    "            batch_size = batch,\n",
    "            class_mode = \"categorical\",\n",
    "            subset = 'training')\n",
    "        \n",
    "        train_steps = train_flow.samples // batch\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\"./best_weights.{epoch:02d}-{loss:.5f}.hdf5\", \n",
    "                                           verbose=1, \n",
    "                                           save_best_only=True,\n",
    "                                           mode=\"auto\")\n",
    "        terminate_onnan = TerminateOnNaN()\n",
    "        reduce_plateau = ReduceLROnPlateau(patience=patience)\n",
    "        \n",
    "        callbacks = [model_checkpoint, terminate_onnan, reduce_plateau]\n",
    "\n",
    "        self.__history = self.model.fit_generator(\n",
    "            train_flow,  \n",
    "            steps_per_epoch=train_steps,\n",
    "            validation_data=self.validation_flow, \n",
    "            validation_steps=self.validation_steps, \n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "\n",
    "        return self.__history\n",
    "\n",
    "    def score(self, X, y=None, **kwargs):\n",
    "        outputs = self.model.evaluate_generator(self.validation_flow, self.validation_steps)\n",
    "        if type(outputs) is not list:\n",
    "            outputs = [outputs]\n",
    "        for name, output in zip(self.model.metrics_names, outputs):\n",
    "            if name == 'acc':\n",
    "                return output\n",
    "        raise Exception('The model is not configured to compute accuracy. '\n",
    "                        'You should pass `metrics=[\"accuracy\"]` to '\n",
    "                        'the `model.compile()` method.')\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.__history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejdscnncgQs4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "def grid_search(create_model, param_grid):\n",
    "    model = KerasBatchClassifier(build_fn=create_model, epochs = 8, batch_size=30)\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, \n",
    "                        param_grid=param_grid, \n",
    "                        cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0))\n",
    "    with tf.device(device):\n",
    "        return grid.fit((1, 1, 1), base_path = base_path, target_size = target_size, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqegaNRbJ5x_"
   },
   "source": [
    "### Definindo a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FD28OW8F1nXw"
   },
   "outputs": [],
   "source": [
    "from classification_models.resnet import ResNet18, preprocess_input\n",
    "\n",
    "def build_resNet18(optimizer='adam'):\n",
    "    model = ResNet18(input_shape = input_shape,\n",
    "                   weights = \"imagenet\",\n",
    "                   include_top=False)\n",
    "\n",
    "    for layer in model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "    output = model.output\n",
    "\n",
    "    output = Flatten(name = 'flat_mdc')(output)\n",
    "\n",
    "    output = Dense(total_classes,\n",
    "                   activation ='softmax',\n",
    "                   name = 'saida_mdc')(output)\n",
    "\n",
    "    model = Model(inputs = model.input, outputs = output)\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy', \n",
    "                  optimizer = optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LjvsKG417IT"
   },
   "outputs": [],
   "source": [
    "def build_resNet18_tuning(optimizer='adam'):\n",
    "    model = build_resNet18()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(loss ='categorical_crossentropy',\n",
    "                  optimizer = optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqdPAGMJ4Sn8"
   },
   "source": [
    "### GridSearch - optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "G1CuiCbX4hHq",
    "outputId": "5579bdd3-4863-414f-caf3-ac592d978bf7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] - epochs= 8\n",
      "[Debug] - batch= 30\n",
      "Found 2492 images belonging to 20 classes.\n",
      "Found 10016 images belonging to 20 classes.\n",
      "Epoch 1/8\n",
      "333/333 [==============================] - 123s 369ms/step - loss: 6.9015 - acc: 0.3511 - val_loss: 13.4125 - val_acc: 0.0908\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 13.41249, saving model to ./best_weights.01-6.90155.hdf5\n",
      "Epoch 2/8\n",
      "333/333 [==============================] - 116s 349ms/step - loss: 2.2572 - acc: 0.5356 - val_loss: 9.0622 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00002: val_loss improved from 13.41249 to 9.06223, saving model to ./best_weights.02-2.25666.hdf5\n",
      "Epoch 3/8\n",
      "147/333 [============>.................] - ETA: 55s - loss: 2.1867 - acc: 0.5335"
     ]
    }
   ],
   "source": [
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid_result = grid_search(build_resNet18_tuning, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaKoggmw4pc7"
   },
   "source": [
    "### Avaliando modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJDkdhhZ4ueT"
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "landmark_recognition-subset_ResNet18_GridSearch_optimizers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
